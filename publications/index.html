<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.ico"> <title>Publications</title> <header> <div class=blog-name ><a href="">Yura Malitsky</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/publications">Publications</a> <li><a href="/blog/">Blog</a> <li><a href="/about/">About</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h3 id=publications ><a href="#publications" class=header-anchor >Publications</a></h3> <table> <tr valign=top > <tr valign=top > <td align=right  class=bibtexnumber > [<a name=alacaoglu2021stochastic >1</a>] <td class=bibtexitem > A.&nbsp;Alacaoglu and Y.&nbsp;Malitsky. Stochastic variance reduction for variational inequality methods, 2021. [&nbsp;<a href="http://arxiv.org/abs/2102.08352">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2020forward >2</a>] <td class=bibtexitem > Y.&nbsp;Malitsky and M.&nbsp;K. Tam. A forward-backward splitting method for monotone inclusions without cocoercivity. <em>SIAM Journal on Optimization</em>, 30(2):1451--1472, 2020. [&nbsp;<a href="http://dx.doi.org/10.1137/18M1207260">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1808.04162">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=mishchenko2020revisiting >3</a>] <td class=bibtexitem > K.&nbsp;Mishchenko, D.&nbsp;Kovalev, E.&nbsp;Shulgin, P.&nbsp;Richt&aacute;rik, and Y.&nbsp;Malitsky. Revisiting stochastic extragradient. In <em>International Conference on Artificial Intelligence and Statistics</em>, 2020. [&nbsp;<a href="http://arxiv.org/abs/1905.11373">arXiv</a>&nbsp;| <a href="http://proceedings.mlr.press/v108/mishchenko20a.html">.html</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=Malitsky2019 >4</a>] <td class=bibtexitem > Y.&nbsp;Malitsky. Golden ratio algorithms for variational inequalities. <em>Mathematical Programming</em>, 184:383--410, 2020. [&nbsp;<a href="http://dx.doi.org/10.1007/s10107-019-01416-w">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1803.08832">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=Malitsky2019_descent >5</a>] <td class=bibtexitem > Y.&nbsp;Malitsky and K.&nbsp;Mishchenko. Adaptive gradient descent without descent. In <em>International Conference on Machine Learning</em>, 2020. [&nbsp;<a href="http://arxiv.org/abs/1910.09529">arXiv</a>&nbsp;| <a href="http://proceedings.mlr.press/v119/malitsky20a.html">.html</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=Ahmet2020adam_type >6</a>] <td class=bibtexitem > A.&nbsp;Alacaoglu, Y.&nbsp;Malitsky, P.&nbsp;Mertikopoulos, and V.&nbsp;Cevher. A new regret analysis for Adam-type algorithms. In <em>International Conference on Machine Learning</em>, 2020. [&nbsp;<a href="http://arxiv.org/abs/2003.09729">arXiv</a>&nbsp;| <a href="http://proceedings.mlr.press/v119/alacaoglu20b.html">.html</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=alacaoglu2020convergence >7</a>] <td class=bibtexitem > A.&nbsp;Alacaoglu, Y.&nbsp;Malitsky, and V.&nbsp;Cevher. Convergence of adaptive algorithms for weakly convex constrained optimization, 2020. [&nbsp;<a href="http://arxiv.org/abs/2006.06650">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=ochs2019model >8</a>] <td class=bibtexitem > Y.&nbsp;Malitsky and P.&nbsp;Ochs. Model function based conditional gradient method with Armijo-like line search. In <em>International Conference on Machine Learning</em>, pages 4891--4900, 2019. [&nbsp;<a href="http://arxiv.org/abs/1901.08087">arXiv</a>&nbsp;| <a href="http://proceedings.mlr.press/v97/ochs19a/ochs19a.pdf">.pdf</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=csetnek2019shadow >9</a>] <td class=bibtexitem > E.&nbsp;R. Csetnek, Y.&nbsp;Malitsky, and M.&nbsp;K. Tam. Shadow Douglas-Rachford splitting for monotone inclusions. <em>Applied Mathematics &amp; Optimization</em>, 80(3):665--678, 2019. [&nbsp;<a href="http://dx.doi.org/10.1007/s00245-019-09597-8">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1903.03393">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2018proximal >10</a>] <td class=bibtexitem > Y.&nbsp;Malitsky. Proximal extrapolated gradient methods for variational inequalities. <em>Optimization Methods and Software</em>, 33(1):140--164, 2018. [&nbsp;<a href="http://dx.doi.org/10.1080/10556788.2017.1300899">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/ 1601.04001">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=luke2018block >11</a>] <td class=bibtexitem > D.&nbsp;R. Luke and Y.&nbsp;Malitsky. Block-coordinate primal-dual method for nonsmooth minimization over linear constraints. In <em>Large-Scale and Distributed Optimization</em>, pages 121--147. Springer, Cham, 2018. [&nbsp;<a href="http://dx.doi.org/10.1007/978-3-319-97478-1_6">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1801.04782">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2018first >12</a>] <td class=bibtexitem > Y.&nbsp;Malitsky and T.&nbsp;Pock. A first-order primal-dual algorithm with linesearch. <em>SIAM Journal on Optimization</em>, 28(1):411--432, 2018. [&nbsp;<a href="http://dx.doi.org/10.1137/16M1092015">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1608.08883">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2017primal >13</a>] <td class=bibtexitem > Y.&nbsp;Malitsky. The primal-dual hybrid gradient method reduces to a primal method for linearly constrained optimization problems, 2017. [&nbsp;<a href="http://arxiv.org/abs/1706.02602">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2015projected >14</a>] <td class=bibtexitem > Y.&nbsp;Malitsky. Projected reflected gradient methods for monotone variational inequalities. <em>SIAM Journal on Optimization</em>, 25(1):502--520, 2015. [&nbsp;<a href="http://dx.doi.org/10.1137/14097238X">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1502.04968">arXiv</a>&nbsp;] <blockquote><font size=-1 > SIAM Student Paper Award, 2015 </font></blockquote> <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2015hybrid >15</a>] <td class=bibtexitem > Y.&nbsp;V. Malitsky and V.&nbsp;Semenov. A hybrid method without extrapolation step for solving variational inequality problems. <em>Journal of Global Optimization</em>, 61(1):193--202, 2015. [&nbsp;<a href="http://dx.doi.org/10.1007/s10898-014-0150-x">DOI</a>&nbsp;| <a href="http://arxiv.org/abs/1501.07298">arXiv</a>&nbsp;] <tr valign=top > <td align=right  class=bibtexnumber > [<a name=malitsky2014extragradient >16</a>] <td class=bibtexitem > Y.&nbsp;V. Malitsky and V.&nbsp;Semenov. An extragradient algorithm for monotone variational inequalities. <em>Cybernetics and Systems Analysis</em>, 50(2):271--277, 2014. [&nbsp;<a href="http://dx.doi.org/10.1007/s10559-014-9614-8">DOI</a>&nbsp;] </table> <div class=page-foot > <div class=copyright > &copy; Yura Malitsky. Last modified: April 20, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>. </div> </div> </div>